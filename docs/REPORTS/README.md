#
## Second Run

Steps:
1. Choose concrete threshold set: `docs/thresholds/sets/TS_<DATE>_<H>_<modelTag>_<note>.yml`
2. Validate with VS Code task: `ts-validate`
3. Print/copy thresholds with VS Code task: `ts-print`
4. Inject values manually for offline run (see strategy/core.py NOTE)
5. Run sandbox, fill new report: `REPORT_2` (copy template)
6. Link chosen TS set in report marker
7. Reference:
  - [THRESHOLDS_SETS.md](../THRESHOLDS_SETS.md)
  - [STABILITY_SCORE.md](../STABILITY_SCORE.md)
  - [REPORT_TEMPLATE.md](../REPORT_TEMPLATE.md)
# MATRIX Reports & Naming Rules

## Report Files
- Human-filled run reports: `docs/REPORTS/REPORT_<YYYYMMDD>_<runTag>.md`
  - Copy from `docs/REPORT_TEMPLATE.md` and fill after each sandbox run.
- Metrics summaries: `docs/summaries/SUMMARY_<YYYYMMDD>_<runTag>.md`
  - Generated by `scripts/metrics/extract_metrics.py`.
- Threshold diffs: `docs/diffs/DIFF_TS_<A>_vs_<B>.md`
  - Generated by `scripts/thresholds/diff_thresholds.py`.

## Example
- `REPORT_20250919_baseline.md` — full run report
- `SUMMARY_20250919_baseline.md` — metrics summary
- `DIFF_TS_20250919_BASE_vs_20250920_GRID.md` — threshold set diff

## Workflow
1. Fill out a run report after each sandbox run.
2. Use metrics extractor to generate summary.
3. Use thresholds diff to compare sets and document changes.
